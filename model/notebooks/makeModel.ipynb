{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import pandas as pd\n",
    " \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9228, 2308, 11536)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indf = pd.read_excel('../data/clean/indf.xlsx').drop('Unnamed: 0', axis=1)\n",
    "outdf = pd.read_excel('../data/clean/outdf.xlsx').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "TEST_SPLIT = int(0.80 * len(indf))\n",
    "X_train, y_train, X_test, y_test = indf.iloc[:TEST_SPLIT], outdf.iloc[:TEST_SPLIT], indf.iloc[TEST_SPLIT:], outdf.iloc[TEST_SPLIT:]\n",
    "len(X_train), len(X_test), len(indf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series date</th>\n",
       "      <th>Length Overall</th>\n",
       "      <th>Maximum Beam</th>\n",
       "      <th>Draft</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>DLR</th>\n",
       "      <th>IMS Division</th>\n",
       "      <th>Dynamic Allowance</th>\n",
       "      <th>Age Allowance</th>\n",
       "      <th>Mainsail measured</th>\n",
       "      <th>...</th>\n",
       "      <th>Mizzen measured</th>\n",
       "      <th>Mizzen rated</th>\n",
       "      <th>Headsail Flying measured</th>\n",
       "      <th>Headsail Flying rated</th>\n",
       "      <th>Asymmetric measured</th>\n",
       "      <th>Asymmetric rated</th>\n",
       "      <th>Quad. Mainsail measured</th>\n",
       "      <th>Quad. Mainsail rated</th>\n",
       "      <th>Mizzen Staysail measured</th>\n",
       "      <th>Mizzen Staysail rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.129929</td>\n",
       "      <td>0.092691</td>\n",
       "      <td>0.166967</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>0.198474</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.880755</td>\n",
       "      <td>0.139393</td>\n",
       "      <td>0.102733</td>\n",
       "      <td>0.062528</td>\n",
       "      <td>0.019525</td>\n",
       "      <td>0.406542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056285</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.050080</td>\n",
       "      <td>0.148463</td>\n",
       "      <td>0.032390</td>\n",
       "      <td>0.135318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940614</td>\n",
       "      <td>0.161120</td>\n",
       "      <td>0.107443</td>\n",
       "      <td>0.207639</td>\n",
       "      <td>0.019245</td>\n",
       "      <td>0.264876</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.964422</td>\n",
       "      <td>0.098186</td>\n",
       "      <td>0.070207</td>\n",
       "      <td>0.155250</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800821</td>\n",
       "      <td>0.044433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043438</td>\n",
       "      <td>0.181478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.964422</td>\n",
       "      <td>0.098186</td>\n",
       "      <td>0.070207</td>\n",
       "      <td>0.155250</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800821</td>\n",
       "      <td>0.044433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043438</td>\n",
       "      <td>0.181478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11531</th>\n",
       "      <td>0.993278</td>\n",
       "      <td>0.114077</td>\n",
       "      <td>0.099711</td>\n",
       "      <td>0.163024</td>\n",
       "      <td>0.009759</td>\n",
       "      <td>0.174742</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133470</td>\n",
       "      <td>0.042830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055708</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.044585</td>\n",
       "      <td>0.186268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11532</th>\n",
       "      <td>0.817285</td>\n",
       "      <td>0.081329</td>\n",
       "      <td>0.061409</td>\n",
       "      <td>0.076386</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.436856</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>0.084603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11533</th>\n",
       "      <td>0.931483</td>\n",
       "      <td>0.134661</td>\n",
       "      <td>0.095001</td>\n",
       "      <td>0.179811</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>0.191282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055785</td>\n",
       "      <td>0.233060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11534</th>\n",
       "      <td>0.926917</td>\n",
       "      <td>0.125631</td>\n",
       "      <td>0.108509</td>\n",
       "      <td>0.183979</td>\n",
       "      <td>0.010565</td>\n",
       "      <td>0.177380</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054677</td>\n",
       "      <td>0.228430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11535</th>\n",
       "      <td>0.948063</td>\n",
       "      <td>0.229278</td>\n",
       "      <td>0.130371</td>\n",
       "      <td>0.357256</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>0.070426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.131112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167105</td>\n",
       "      <td>0.495389</td>\n",
       "      <td>0.107078</td>\n",
       "      <td>0.447353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11536 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Series date  Length Overall  Maximum Beam     Draft  Displacement  \\\n",
       "0         0.926917        0.129929      0.092691  0.166967      0.012646   \n",
       "1         0.880755        0.139393      0.102733  0.062528      0.019525   \n",
       "2         0.940614        0.161120      0.107443  0.207639      0.019245   \n",
       "3         0.964422        0.098186      0.070207  0.155250      0.004272   \n",
       "4         0.964422        0.098186      0.070207  0.155250      0.004272   \n",
       "...            ...             ...           ...       ...           ...   \n",
       "11531     0.993278        0.114077      0.099711  0.163024      0.009759   \n",
       "11532     0.817285        0.081329      0.061409  0.076386      0.008794   \n",
       "11533     0.931483        0.134661      0.095001  0.179811      0.010956   \n",
       "11534     0.926917        0.125631      0.108509  0.183979      0.010565   \n",
       "11535     0.948063        0.229278      0.130371  0.357256      0.019176   \n",
       "\n",
       "            DLR  IMS Division  Dynamic Allowance  Age Allowance  \\\n",
       "0      0.198474      0.666667           0.200000       1.000000   \n",
       "1      0.406542      0.000000           0.800000       1.000000   \n",
       "2      0.264876      0.666667           0.000000       1.000000   \n",
       "3      0.078507      1.000000           0.000000       0.800821   \n",
       "4      0.078507      1.000000           0.000000       0.800821   \n",
       "...         ...           ...                ...            ...   \n",
       "11531  0.174742      1.000000           0.000000       0.133470   \n",
       "11532  0.436856      0.666667           0.680000       1.000000   \n",
       "11533  0.191282      1.000000           0.000000       1.000000   \n",
       "11534  0.177380      0.666667           0.146667       1.000000   \n",
       "11535  0.070426      1.000000           0.000000       1.000000   \n",
       "\n",
       "       Mainsail measured  ...  Mizzen measured  Mizzen rated  \\\n",
       "0               0.049676  ...         0.000000      0.000000   \n",
       "1               0.031359  ...         0.056285      0.056638   \n",
       "2               0.069499  ...         0.000000      0.000000   \n",
       "3               0.044433  ...         0.000000      0.000000   \n",
       "4               0.044433  ...         0.000000      0.000000   \n",
       "...                  ...  ...              ...           ...   \n",
       "11531           0.042830  ...         0.000000      0.000000   \n",
       "11532           0.027189  ...         0.000000      0.000000   \n",
       "11533           0.055850  ...         0.000000      0.000000   \n",
       "11534           0.051593  ...         0.000000      0.000000   \n",
       "11535           0.131112  ...         0.000000      0.000000   \n",
       "\n",
       "       Headsail Flying measured  Headsail Flying rated  Asymmetric measured  \\\n",
       "0                      0.000000               0.000000             0.000000   \n",
       "1                      0.050080               0.148463             0.032390   \n",
       "2                      0.000000               0.000000             0.000000   \n",
       "3                      0.000000               0.000000             0.043438   \n",
       "4                      0.000000               0.000000             0.043438   \n",
       "...                         ...                    ...                  ...   \n",
       "11531                  0.055708               0.165149             0.044585   \n",
       "11532                  0.000000               0.000000             0.020251   \n",
       "11533                  0.000000               0.000000             0.055785   \n",
       "11534                  0.000000               0.000000             0.054677   \n",
       "11535                  0.167105               0.495389             0.107078   \n",
       "\n",
       "       Asymmetric rated  Quad. Mainsail measured  Quad. Mainsail rated  \\\n",
       "0              0.000000                      0.0                   0.0   \n",
       "1              0.135318                      0.0                   0.0   \n",
       "2              0.000000                      0.0                   0.0   \n",
       "3              0.181478                      0.0                   0.0   \n",
       "4              0.181478                      0.0                   0.0   \n",
       "...                 ...                      ...                   ...   \n",
       "11531          0.186268                      0.0                   0.0   \n",
       "11532          0.084603                      0.0                   0.0   \n",
       "11533          0.233060                      0.0                   0.0   \n",
       "11534          0.228430                      0.0                   0.0   \n",
       "11535          0.447353                      0.0                   0.0   \n",
       "\n",
       "       Mizzen Staysail measured  Mizzen Staysail rated  \n",
       "0                           0.0                    0.0  \n",
       "1                           0.0                    0.0  \n",
       "2                           0.0                    0.0  \n",
       "3                           0.0                    0.0  \n",
       "4                           0.0                    0.0  \n",
       "...                         ...                    ...  \n",
       "11531                       0.0                    0.0  \n",
       "11532                       0.0                    0.0  \n",
       "11533                       0.0                    0.0  \n",
       "11534                       0.0                    0.0  \n",
       "11535                       0.0                    0.0  \n",
       "\n",
       "[11536 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beat Angles 6 kt</th>\n",
       "      <th>Beat Angles 8 kt</th>\n",
       "      <th>Beat Angles 10 kt</th>\n",
       "      <th>Beat Angles 12 kt</th>\n",
       "      <th>Beat Angles 14 kt</th>\n",
       "      <th>Beat Angles 16 kt</th>\n",
       "      <th>Beat Angles 20 kt</th>\n",
       "      <th>Beat Angles 24 kt</th>\n",
       "      <th>Beat VMG 6 kt</th>\n",
       "      <th>Beat VMG 8 kt</th>\n",
       "      <th>...</th>\n",
       "      <th>Run VMG 20 kt</th>\n",
       "      <th>Run VMG 24 kt</th>\n",
       "      <th>Gybe Angles 6 kt</th>\n",
       "      <th>Gybe Angles 8 kt</th>\n",
       "      <th>Gybe Angles 10 kt</th>\n",
       "      <th>Gybe Angles 12 kt</th>\n",
       "      <th>Gybe Angles 14 kt</th>\n",
       "      <th>Gybe Angles 16 kt</th>\n",
       "      <th>Gybe Angles 20 kt</th>\n",
       "      <th>Gybe Angles 24 kt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.900002</td>\n",
       "      <td>40.799999</td>\n",
       "      <td>39.099998</td>\n",
       "      <td>37.799999</td>\n",
       "      <td>37.099998</td>\n",
       "      <td>36.599998</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>36.599998</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.58</td>\n",
       "      <td>...</td>\n",
       "      <td>8.46</td>\n",
       "      <td>9.370000</td>\n",
       "      <td>140.800003</td>\n",
       "      <td>146.899994</td>\n",
       "      <td>150.500000</td>\n",
       "      <td>155.800003</td>\n",
       "      <td>165.100006</td>\n",
       "      <td>176.800003</td>\n",
       "      <td>177.699997</td>\n",
       "      <td>176.399994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.400002</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>47.200001</td>\n",
       "      <td>47.200001</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>48.200001</td>\n",
       "      <td>49.599998</td>\n",
       "      <td>51.700001</td>\n",
       "      <td>2.63</td>\n",
       "      <td>3.24</td>\n",
       "      <td>...</td>\n",
       "      <td>7.71</td>\n",
       "      <td>8.320000</td>\n",
       "      <td>144.800003</td>\n",
       "      <td>148.699997</td>\n",
       "      <td>152.500000</td>\n",
       "      <td>156.800003</td>\n",
       "      <td>165.100006</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.500000</td>\n",
       "      <td>40.200001</td>\n",
       "      <td>38.599998</td>\n",
       "      <td>37.799999</td>\n",
       "      <td>37.299999</td>\n",
       "      <td>37.099998</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.96</td>\n",
       "      <td>...</td>\n",
       "      <td>8.70</td>\n",
       "      <td>9.510000</td>\n",
       "      <td>141.399994</td>\n",
       "      <td>146.500000</td>\n",
       "      <td>150.699997</td>\n",
       "      <td>155.600006</td>\n",
       "      <td>163.300003</td>\n",
       "      <td>175.399994</td>\n",
       "      <td>178.300003</td>\n",
       "      <td>178.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.599998</td>\n",
       "      <td>40.200001</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>38.700001</td>\n",
       "      <td>38.400002</td>\n",
       "      <td>38.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>11.50</td>\n",
       "      <td>14.570000</td>\n",
       "      <td>143.100006</td>\n",
       "      <td>145.800003</td>\n",
       "      <td>149.699997</td>\n",
       "      <td>149.600006</td>\n",
       "      <td>146.500000</td>\n",
       "      <td>145.699997</td>\n",
       "      <td>146.199997</td>\n",
       "      <td>150.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.599998</td>\n",
       "      <td>40.200001</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>38.700001</td>\n",
       "      <td>38.400002</td>\n",
       "      <td>38.200001</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>11.50</td>\n",
       "      <td>14.570000</td>\n",
       "      <td>143.100006</td>\n",
       "      <td>145.800003</td>\n",
       "      <td>149.699997</td>\n",
       "      <td>149.600006</td>\n",
       "      <td>146.500000</td>\n",
       "      <td>145.699997</td>\n",
       "      <td>146.199997</td>\n",
       "      <td>150.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11531</th>\n",
       "      <td>44.099998</td>\n",
       "      <td>41.700001</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>38.400002</td>\n",
       "      <td>38.299999</td>\n",
       "      <td>39.200001</td>\n",
       "      <td>40.400002</td>\n",
       "      <td>3.53</td>\n",
       "      <td>4.33</td>\n",
       "      <td>...</td>\n",
       "      <td>8.27</td>\n",
       "      <td>10.320000</td>\n",
       "      <td>143.300003</td>\n",
       "      <td>146.899994</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>149.399994</td>\n",
       "      <td>148.399994</td>\n",
       "      <td>146.600006</td>\n",
       "      <td>143.100006</td>\n",
       "      <td>139.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11532</th>\n",
       "      <td>43.799999</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>41.299999</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>39.900002</td>\n",
       "      <td>39.599998</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>40.900002</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.56</td>\n",
       "      <td>...</td>\n",
       "      <td>6.98</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>145.300003</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>153.800003</td>\n",
       "      <td>159.399994</td>\n",
       "      <td>173.600006</td>\n",
       "      <td>178.100006</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>179.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11533</th>\n",
       "      <td>42.400002</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>38.200001</td>\n",
       "      <td>37.400002</td>\n",
       "      <td>37.099998</td>\n",
       "      <td>37.299999</td>\n",
       "      <td>37.400002</td>\n",
       "      <td>38.900002</td>\n",
       "      <td>3.99</td>\n",
       "      <td>4.80</td>\n",
       "      <td>...</td>\n",
       "      <td>8.90</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>146.199997</td>\n",
       "      <td>149.300003</td>\n",
       "      <td>151.300003</td>\n",
       "      <td>149.399994</td>\n",
       "      <td>146.699997</td>\n",
       "      <td>143.600006</td>\n",
       "      <td>144.199997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11534</th>\n",
       "      <td>42.799999</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>38.799999</td>\n",
       "      <td>37.599998</td>\n",
       "      <td>37.599998</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>38.700001</td>\n",
       "      <td>3.77</td>\n",
       "      <td>4.61</td>\n",
       "      <td>...</td>\n",
       "      <td>8.91</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>143.100006</td>\n",
       "      <td>146.500000</td>\n",
       "      <td>148.899994</td>\n",
       "      <td>150.600006</td>\n",
       "      <td>148.600006</td>\n",
       "      <td>146.500000</td>\n",
       "      <td>143.300003</td>\n",
       "      <td>143.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11535</th>\n",
       "      <td>41.900002</td>\n",
       "      <td>39.799999</td>\n",
       "      <td>38.400002</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>36.900002</td>\n",
       "      <td>5.62</td>\n",
       "      <td>6.29</td>\n",
       "      <td>...</td>\n",
       "      <td>13.27</td>\n",
       "      <td>16.360001</td>\n",
       "      <td>140.600006</td>\n",
       "      <td>142.899994</td>\n",
       "      <td>147.800003</td>\n",
       "      <td>151.600006</td>\n",
       "      <td>147.399994</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>146.199997</td>\n",
       "      <td>149.300003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11536 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Beat Angles 6 kt  Beat Angles 8 kt  Beat Angles 10 kt  \\\n",
       "0             42.900002         40.799999          39.099998   \n",
       "1             49.400002         48.000000          47.200001   \n",
       "2             42.500000         40.200001          38.599998   \n",
       "3             42.599998         40.200001          38.799999   \n",
       "4             42.599998         40.200001          38.799999   \n",
       "...                 ...               ...                ...   \n",
       "11531         44.099998         41.700001          40.000000   \n",
       "11532         43.799999         42.000000          41.299999   \n",
       "11533         42.400002         40.000000          38.200001   \n",
       "11534         42.799999         40.599998          38.799999   \n",
       "11535         41.900002         39.799999          38.400002   \n",
       "\n",
       "       Beat Angles 12 kt  Beat Angles 14 kt  Beat Angles 16 kt  \\\n",
       "0              37.799999          37.099998          36.599998   \n",
       "1              47.200001          47.500000          48.200001   \n",
       "2              37.799999          37.299999          37.099998   \n",
       "3              38.700001          38.400002          38.200001   \n",
       "4              38.700001          38.400002          38.200001   \n",
       "...                  ...                ...                ...   \n",
       "11531          38.900002          38.400002          38.299999   \n",
       "11532          40.599998          39.900002          39.599998   \n",
       "11533          37.400002          37.099998          37.299999   \n",
       "11534          37.599998          37.599998          37.500000   \n",
       "11535          37.700001          37.000000          36.400002   \n",
       "\n",
       "       Beat Angles 20 kt  Beat Angles 24 kt  Beat VMG 6 kt  Beat VMG 8 kt  \\\n",
       "0              36.200001          36.599998           3.74           4.58   \n",
       "1              49.599998          51.700001           2.63           3.24   \n",
       "2              37.500000          38.500000           4.14           4.96   \n",
       "3              38.900002          40.599998           4.08           4.85   \n",
       "4              38.900002          40.599998           4.08           4.85   \n",
       "...                  ...                ...            ...            ...   \n",
       "11531          39.200001          40.400002           3.53           4.33   \n",
       "11532          39.799999          40.900002           2.96           3.56   \n",
       "11533          37.400002          38.900002           3.99           4.80   \n",
       "11534          37.500000          38.700001           3.77           4.61   \n",
       "11535          36.200001          36.900002           5.62           6.29   \n",
       "\n",
       "       ...  Run VMG 20 kt  Run VMG 24 kt  Gybe Angles 6 kt  Gybe Angles 8 kt  \\\n",
       "0      ...           8.46       9.370000        140.800003        146.899994   \n",
       "1      ...           7.71       8.320000        144.800003        148.699997   \n",
       "2      ...           8.70       9.510000        141.399994        146.500000   \n",
       "3      ...          11.50      14.570000        143.100006        145.800003   \n",
       "4      ...          11.50      14.570000        143.100006        145.800003   \n",
       "...    ...            ...            ...               ...               ...   \n",
       "11531  ...           8.27      10.320000        143.300003        146.899994   \n",
       "11532  ...           6.98       7.520000        145.300003        151.000000   \n",
       "11533  ...           8.90      11.450000        143.000000        146.199997   \n",
       "11534  ...           8.91      11.580000        143.100006        146.500000   \n",
       "11535  ...          13.27      16.360001        140.600006        142.899994   \n",
       "\n",
       "       Gybe Angles 10 kt  Gybe Angles 12 kt  Gybe Angles 14 kt  \\\n",
       "0             150.500000         155.800003         165.100006   \n",
       "1             152.500000         156.800003         165.100006   \n",
       "2             150.699997         155.600006         163.300003   \n",
       "3             149.699997         149.600006         146.500000   \n",
       "4             149.699997         149.600006         146.500000   \n",
       "...                  ...                ...                ...   \n",
       "11531         148.000000         149.399994         148.399994   \n",
       "11532         153.800003         159.399994         173.600006   \n",
       "11533         149.300003         151.300003         149.399994   \n",
       "11534         148.899994         150.600006         148.600006   \n",
       "11535         147.800003         151.600006         147.399994   \n",
       "\n",
       "       Gybe Angles 16 kt  Gybe Angles 20 kt  Gybe Angles 24 kt  \n",
       "0             176.800003         177.699997         176.399994  \n",
       "1             175.000000         179.000000         179.000000  \n",
       "2             175.399994         178.300003         178.100006  \n",
       "3             145.699997         146.199997         150.300003  \n",
       "4             145.699997         146.199997         150.300003  \n",
       "...                  ...                ...                ...  \n",
       "11531         146.600006         143.100006         139.500000  \n",
       "11532         178.100006         179.000000         179.000000  \n",
       "11533         146.699997         143.600006         144.199997  \n",
       "11534         146.500000         143.300003         143.500000  \n",
       "11535         145.000000         146.199997         149.300003  \n",
       "\n",
       "[11536 rows x 96 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9269, 0.1299, 0.0927, 0.1670, 0.0126, 0.1985, 0.6667, 0.2000, 1.0000,\n",
       "        0.0497, 0.0446, 0.0445, 0.0651, 0.2816, 0.2816, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "if not os.path.exists('../models'):\n",
    "    os.mkdir('../models')\n",
    "    \n",
    "tesatTensor = torch.tensor(indf.iloc[0].values, dtype=torch.float32)\n",
    "tesatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "col0 = outdf.columns.tolist()[0]\n",
    "\n",
    "# making dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, input_df, output_df):\n",
    "        self.input_df = input_df \n",
    "        self.output_df = output_df \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.input_df.iloc[idx].values \n",
    "        output_data = self.output_df.iloc[idx] \n",
    "        \n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "        output_tensor = torch.tensor(output_data, dtype=torch.float32)\n",
    "        return input_tensor, output_tensor\n",
    "    \n",
    "train_dataset = MyDataset(X_train, y_train[col0])\n",
    "test_dataset = MyDataset(X_test, y_test[col0]) \n",
    "\n",
    "# making dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=32,\n",
    "                              shuffle=True, \n",
    "                              num_workers=os.cpu_count())\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                             batch_size=32, \n",
    "                             shuffle=False, \n",
    "                             num_workers=os.cpu_count())\n",
    "\n",
    "# test \n",
    "print(len(train_dataloader), len(test_dataloader))\n",
    "x, y = next(iter(train_dataloader))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/miniconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss = 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X).squeeze()\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    return test_loss\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss = 0 \n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X).squeeze()\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          epochs: int, \n",
    "          device: torch.device = device):\n",
    "    results = {\"train_loss\": [], \n",
    "               \"test_loss\": []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model=model, \n",
    "                                dataloader=train_dataloader, \n",
    "                                loss_fn=loss_fn, \n",
    "                                optimizer=optimizer,\n",
    "                                device=device)\n",
    "        test_loss = test_step(model=model, \n",
    "                              dataloader=test_dataloader, \n",
    "                              loss_fn=loss_fn,\n",
    "                              device=device)\n",
    "        \n",
    "        print(f\"epoch: {epoch}, train_loss: {train_loss:.4f}, test_loss: {test_loss:.4f}\")\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['test_loss'].append(test_loss)\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(res):\n",
    "    test_loss = res['test_loss']\n",
    "    train_loss = res['train_loss'] \n",
    "    epochs = range(len(train_loss))\n",
    "    \n",
    "    plt.figure(figsize=(10, 7)) \n",
    "    plt.plot(epochs, train_loss, label=\"train_loss\") \n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\") \n",
    "    plt.title(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_results_since(res, since):\n",
    "    test_loss = res['test_loss'][since:]\n",
    "    train_loss = res['train_loss'][since:]\n",
    "    epochs = range(len(train_loss))\n",
    "    \n",
    "    plt.figure(figsize=(10, 7)) \n",
    "    plt.plot(epochs, train_loss, label=\"train_loss\") \n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\") \n",
    "    plt.title(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class yachtModel0(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS = 32\n",
    "OUT_FEATURES = 1\n",
    "EPOCHS = 50\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model0 = yachtModel0(input_shape=IN_FEATURES, \n",
    "                     hidden_units=HIDDEN_UNITS, \n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model0.parameters(),\n",
    "                             lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0_results = train(model=model0, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model0_results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel1(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units2),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 64\n",
    "HIDDEN_UNITS2 = 32\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model1 = yachtModel1(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model1.parameters(),\n",
    "                             lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_results = train(model=model1, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model1_results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel2(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 64\n",
    "HIDDEN_UNITS2 = 32\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model2 = yachtModel2(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model2.parameters(),\n",
    "                             lr=0.00015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_results = train(model=model2, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model2_results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel3(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 32\n",
    "HIDDEN_UNITS2 = 32\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model3 = yachtModel3(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model3.parameters(),\n",
    "                             lr=0.00015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_results = train(model=model3, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model3_results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel4(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 128\n",
    "HIDDEN_UNITS2 = 64\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model4 = yachtModel4(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model4.parameters(),\n",
    "                             lr=0.00015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_results = train(model=model4, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model4_results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying different loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel5(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 128\n",
    "HIDDEN_UNITS2 = 64\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model5 = yachtModel5(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(params=model5.parameters(),\n",
    "                             lr=0.00015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_results = train(model=model5, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model5_results, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss = 0 \n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X).view(-1)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel6(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 128\n",
    "HIDDEN_UNITS2 = 64\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model6 = yachtModel6(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.HuberLoss()\n",
    "optimizer = torch.optim.Adam(params=model6.parameters(),\n",
    "                             lr=0.00015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_results = train(model=model6, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model6_results, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel7(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 64\n",
    "HIDDEN_UNITS2 = 32\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model7 = yachtModel7(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.HuberLoss()\n",
    "optimizer = torch.optim.Adam(params=model7.parameters(),\n",
    "                             lr=0.00015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7_results = train(model=model7, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model7_results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results\n",
    "model6 is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = len(indf.columns)\n",
    "HIDDEN_UNITS1 = 128\n",
    "HIDDEN_UNITS2 = 64\n",
    "OUT_FEATURES = 1\n",
    "\n",
    "torch.manual_seed(42) \n",
    "model = yachtModel6(input_shape=IN_FEATURES, \n",
    "                     hidden_units1=HIDDEN_UNITS1,\n",
    "                     hidden_units2=HIDDEN_UNITS2,\n",
    "                     output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "loss_fun = nn.HuberLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=0.0001)\n",
    "\n",
    "model_results = train(model=model, \n",
    "                       train_dataloader=train_dataloader, \n",
    "                       test_dataloader=test_dataloader, \n",
    "                       loss_fn=loss_fun, \n",
    "                       optimizer=optimizer, \n",
    "                       epochs=EPOCHS, \n",
    "                       device=device)\n",
    "\n",
    "plot_results_since(model_results, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss = 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X).squeeze()\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    return test_loss\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device=device):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss = 0 \n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X).squeeze()\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          epochs: int, \n",
    "          device: torch.device = device):\n",
    "    results = {\"train_loss\": [], \n",
    "               \"test_loss\": []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_step(model=model, \n",
    "                                dataloader=train_dataloader, \n",
    "                                loss_fn=loss_fn, \n",
    "                                optimizer=optimizer,\n",
    "                                device=device)\n",
    "        test_loss = test_step(model=model, \n",
    "                              dataloader=test_dataloader, \n",
    "                              loss_fn=loss_fn,\n",
    "                              device=device)\n",
    "        \n",
    "        # print(f\"epoch: {epoch}, train_loss: {train_loss:.4f}, test_loss: {test_loss:.4f}\")\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['test_loss'].append(test_loss)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models( X_train: pd.DataFrame, \n",
    "                  y_train: pd.DataFrame,\n",
    "                  X_test: pd.DataFrame, \n",
    "                  y_test: pd.DataFrame,\n",
    "                  device: torch.device = device):\n",
    "  \n",
    "    models = {}\n",
    "    final_results = {\"train_loss\": [], \n",
    "               \"test_loss\": []}\n",
    "    \n",
    "    if not os.path.exists('../models'):\n",
    "        os.makedirs('../models', exist_ok=True)\n",
    "     \n",
    "    for column in tqdm(y_train.columns.to_list()):\n",
    "        # making datasets \n",
    "        train_dataset = MyDataset(X_train, y_train[column])\n",
    "        test_dataset = MyDataset(X_test, y_test[column]) \n",
    "\n",
    "        # making dataloader\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=32,\n",
    "                                        shuffle=True, \n",
    "                                        num_workers=os.cpu_count())\n",
    "        test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                                    batch_size=32, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=os.cpu_count())\n",
    "        \n",
    "        # making model\n",
    "        IN_FEATURES = len(indf.columns)\n",
    "        HIDDEN_UNITS1 = 128\n",
    "        HIDDEN_UNITS2 = 64\n",
    "        OUT_FEATURES = 1\n",
    "\n",
    "        torch.manual_seed(42) \n",
    "        models[column] = yachtModel6(input_shape=IN_FEATURES, \n",
    "                        hidden_units1=HIDDEN_UNITS1,\n",
    "                        hidden_units2=HIDDEN_UNITS2,\n",
    "                        output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "        loss_fun = nn.HuberLoss()\n",
    "        optimizer = torch.optim.Adam(params=models[column].parameters(),\n",
    "                                    lr=0.0001)\n",
    "\n",
    "        model_results = train(model=models[column], \n",
    "                                train_dataloader=train_dataloader, \n",
    "                                test_dataloader=test_dataloader, \n",
    "                                loss_fn=loss_fun, \n",
    "                                optimizer=optimizer, \n",
    "                                epochs=EPOCHS, \n",
    "                                device=device)\n",
    "        final_results[\"train_loss\"].append(model_results[\"train_loss\"][-1])\n",
    "        final_results[\"test_loss\"].append(model_results[\"test_loss\"][-1])\n",
    "        \n",
    "        # saving model\n",
    "        torch.save(models[column].state_dict(), \"../models/\" + str(column))\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "final_results = train_models(X_train=X_train,\n",
    "                             y_train=y_train,\n",
    "                             X_test=X_test, \n",
    "                             y_test=y_test,\n",
    "                             device=device)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [0.21377085128663734, 0.2387742920678792, 0.2547902051346525, 0.3342327076980399, 0.41784169986380015, 0.49190813674233774, 0.6641346549286562, 0.97396822309824, 0.011800959767535278, 0.012237605003955867, 0.012924017412040164, 0.014949936685587398, 0.016897683798828545, 0.019506099891678067, 0.0317702452429799, 0.04443796769841526, 0.024112555158089292, 0.021208747283629686, 0.018552819131567404, 0.018622953025785285, 0.02013825598595, 0.023151574994737096, 0.04460553407578836, 0.068650345253568, 0.025382486404045435, 0.020984433578475535, 0.01781222696664218, 0.0190491948105962, 0.021567729691681563, 0.02562138945457226, 0.034768545624693684, 0.044107416625357006, 0.026909825956017087, 0.0212822955127509, 0.019787666299235347, 0.023939730457729534, 0.02824812754827542, 0.03401160446578914, 0.046924015964706876, 0.05895793720254849, 0.024374288390793395, 0.019506749416666046, 0.02015631235362543, 0.029023623920551737, 0.03908993648751848, 0.048522686761414, 0.06779429339078677, 0.08894923818539377, 0.02913596957058333, 0.02770875879018777, 0.025951867571880453, 0.03193492129296838, 0.0423451359367649, 0.059011131215673, 0.10692337535605298, 0.17086055208397158, 0.03315408578767702, 0.0335351622995363, 0.03234607375075454, 0.04123589467212145, 0.05403853390224046, 0.07021939698413375, 0.12012809485351751, 0.21374324680818407, 0.03277735394718944, 0.038354255636077646, 0.03752131551297891, 0.04762731527000872, 0.07368751339433928, 0.11241618583178643, 0.21174118268242345, 0.31982804025333234, 0.0232020026849499, 0.030162519256497336, 0.03390356336613943, 0.03670076566029688, 0.04563492728932197, 0.0745950212096312, 0.22013301899895124, 0.47918935888366304, 0.017123934456749874, 0.021407312919807887, 0.021623010365260206, 0.02130558166111526, 0.027069296367195753, 0.04514180519698658, 0.14523260635049903, 0.34726313620187005, 1.1171345769534062, 1.5474560155588037, 2.8205150629822358, 4.487242918113524, 5.614597092037795, 4.3588260296719294, 5.282161753895373, 7.998074975393223], 'test_loss': [0.2024157816314534, 0.26251836998821937, 0.304111876103976, 0.40661603334831864, 0.5194250847378822, 0.6182101478111254, 0.8234546792833772, 1.162722794568702, 0.012777507158430063, 0.012600657743822834, 0.01330659357346084, 0.015167913865298033, 0.017530368854712746, 0.020142251970118857, 0.027132693001974937, 0.041200987522630655, 0.025872595289288317, 0.020033043946381914, 0.01617463089984982, 0.01583866285134668, 0.018011364680778696, 0.02108652603952852, 0.029571307159654083, 0.04361793865794188, 0.027298401122632092, 0.019321768295836365, 0.015442672415240987, 0.016526758071188242, 0.020211452347137136, 0.0248899106574181, 0.03372062330631769, 0.04206240868629658, 0.02833750895033144, 0.019997314061995033, 0.01860889666139671, 0.022787089568039733, 0.02907900013386795, 0.03688327872436749, 0.0529137481871533, 0.06662611540866224, 0.02691907008267837, 0.02035283839508687, 0.022486497724607382, 0.03102762747734581, 0.043253602778972826, 0.05588639766130954, 0.08292283394616352, 0.11231098671073783, 0.03725972400391347, 0.03186643351992107, 0.030414507035421184, 0.0388818683664072, 0.053788345906730387, 0.0737451545077644, 0.13645109567434005, 0.2172011840221, 0.040857085945365364, 0.03908437087315403, 0.0377723491712384, 0.0477680087708294, 0.06581063900296003, 0.08915374966414824, 0.15531861577948478, 0.2583170356611683, 0.039901908703964865, 0.04684747579469256, 0.046596263969087434, 0.06227849520844956, 0.10175451087727122, 0.15272489889231447, 0.266128747532629, 0.37896670850172437, 0.0276471741548548, 0.03617654610680391, 0.039249198842946795, 0.04256194776357853, 0.06059281810582297, 0.11201565141138965, 0.34617616751626745, 0.6844394692819412, 0.020653176083140178, 0.026452101773442063, 0.02683660959544247, 0.02722875516198269, 0.03852931794085323, 0.0703316065419006, 0.23694912979557906, 0.5141240799263732, 1.2656707661609128, 1.5751902750910145, 3.0375846950975185, 4.955871954356154, 6.523851675530002, 6.5829699153769505, 8.252739096341069, 8.764927537473914]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 96 artists>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(final_results)\n",
    "plt.bar(range(len(final_results['test_loss'])), final_results['test_loss']) # predictions for last 8 looks bad, let"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 6 kt</th>\n",
       "      <td>129.500000</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 8 kt</th>\n",
       "      <td>138.800003</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 10 kt</th>\n",
       "      <td>139.600006</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 12 kt</th>\n",
       "      <td>136.000000</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 14 kt</th>\n",
       "      <td>133.199997</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 16 kt</th>\n",
       "      <td>134.699997</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 20 kt</th>\n",
       "      <td>132.399994</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gybe Angles 24 kt</th>\n",
       "      <td>132.699997</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0      1\n",
       "Gybe Angles 6 kt   129.500000  180.0\n",
       "Gybe Angles 8 kt   138.800003  180.0\n",
       "Gybe Angles 10 kt  139.600006  180.0\n",
       "Gybe Angles 12 kt  136.000000  180.0\n",
       "Gybe Angles 14 kt  133.199997  180.0\n",
       "Gybe Angles 16 kt  134.699997  180.0\n",
       "Gybe Angles 20 kt  132.399994  180.0\n",
       "Gybe Angles 24 kt  132.699997  180.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([outdf[outdf.columns.to_list()[-8:]].min(), \n",
    "              outdf[outdf.columns.to_list()[-8:]].max()],\n",
    "          axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train2(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module, \n",
    "          optimizer: torch.optim.Optimizer, \n",
    "          epochs: int, \n",
    "          lr_start: int,\n",
    "          lr_end: int,\n",
    "          device: torch.device = device):\n",
    "    \n",
    "    def lr_lambda(epoch):\n",
    "        return (lr_end / lr_start) ** (epoch / EPOCHS)\n",
    "    \n",
    "    results = {\"train_loss\": [], \n",
    "               \"test_loss\": []}\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_step(model=model, \n",
    "                                dataloader=train_dataloader, \n",
    "                                loss_fn=loss_fn, \n",
    "                                optimizer=optimizer,\n",
    "                                device=device)\n",
    "        test_loss = test_step(model=model, \n",
    "                              dataloader=test_dataloader, \n",
    "                              loss_fn=loss_fn,\n",
    "                              device=device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"epoch: {epoch}, train_loss: {train_loss:.4f}, test_loss: {test_loss:.4f}\")\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['test_loss'].append(test_loss)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 41.3045, test_loss: 10.2907\n",
      "epoch: 1, train_loss: 8.5223, test_loss: 8.8348\n",
      "epoch: 2, train_loss: 7.7021, test_loss: 7.8635\n",
      "epoch: 3, train_loss: 6.8762, test_loss: 7.1116\n",
      "epoch: 4, train_loss: 6.4266, test_loss: 7.3801\n"
     ]
    }
   ],
   "source": [
    "class yachtModel8(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "def train_models2( X_train: pd.DataFrame, \n",
    "                  y_train: pd.DataFrame,\n",
    "                  X_test: pd.DataFrame, \n",
    "                  y_test: pd.DataFrame,\n",
    "                  device: torch.device = device):\n",
    "  \n",
    "    models = {}\n",
    "    final_results2 = {\"train_loss\": [], \n",
    "               \"test_loss\": []}\n",
    "     \n",
    "    for column in [y_train.columns.to_list()[-1]]:\n",
    "        # making datasets \n",
    "        train_dataset = MyDataset(X_train, y_train[column])\n",
    "        test_dataset = MyDataset(X_test, y_test[column]) \n",
    "\n",
    "        # making dataloader\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=32,\n",
    "                                        shuffle=True, \n",
    "                                        num_workers=os.cpu_count())\n",
    "        test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                                    batch_size=32, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=os.cpu_count())\n",
    "        \n",
    "        # making model\n",
    "        IN_FEATURES = len(indf.columns)\n",
    "        HIDDEN_UNITS1 = 128\n",
    "        HIDDEN_UNITS2 = 64\n",
    "        OUT_FEATURES = 1\n",
    "        EPOCHS=5\n",
    "        start_lr = 0.001\n",
    "        end_lr = 0.0001\n",
    "        \n",
    "\n",
    "        torch.manual_seed(42) \n",
    "        models[column] = yachtModel8(input_shape=IN_FEATURES, # this is with relu\n",
    "                        hidden_units1=HIDDEN_UNITS1,\n",
    "                        hidden_units2=HIDDEN_UNITS2,\n",
    "                        output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "        loss_fun = nn.HuberLoss()\n",
    "        optimizer = torch.optim.Adam(params=models[column].parameters(),\n",
    "                                    lr=start_lr)\n",
    "\n",
    "        model_results = train2(model=models[column], \n",
    "                                train_dataloader=train_dataloader, \n",
    "                                test_dataloader=test_dataloader, \n",
    "                                loss_fn=loss_fun, \n",
    "                                optimizer=optimizer, \n",
    "                                epochs=EPOCHS,\n",
    "                                lr_start=start_lr,\n",
    "                                lr_end=end_lr, \n",
    "                                device=device)\n",
    "        final_results2[\"train_loss\"].append(model_results[\"train_loss\"][-1])\n",
    "        final_results2[\"test_loss\"].append(model_results[\"test_loss\"][-1])\n",
    "        \n",
    "        # saving model\n",
    "        torch.save(models[column].state_dict(), \"../models/\" + str(column))\n",
    "    \n",
    "    return final_results2\n",
    "\n",
    "final_results2 = train_models2(X_train=X_train,\n",
    "                             y_train=y_train,\n",
    "                             X_test=X_test, \n",
    "                             y_test=y_test,\n",
    "                             device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cel above I found best nn setings for last elemnet, now i will apply it to last row (last 8 elements) i have to predict\n",
    "protips for future: when you serach for best setting it's good to make extra one cell with best setting for later to not loss it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class yachtModel8(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units1: int, \n",
    "                 hidden_units2: int, \n",
    "                 output_shape: int): \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape, out_features= hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units2, out_features=hidden_units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units1, out_features=output_shape), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x) \n",
    "    \n",
    "def train_models2( X_train: pd.DataFrame, \n",
    "                  y_train: pd.DataFrame,\n",
    "                  X_test: pd.DataFrame, \n",
    "                  y_test: pd.DataFrame,\n",
    "                  device: torch.device = device):\n",
    "  \n",
    "    models = {}\n",
    "    final_results2 = {\"train_loss\": [], \n",
    "               \"test_loss\": []}\n",
    "     \n",
    "    for column in y_train.columns.to_list()[-8:]:\n",
    "        # making datasets \n",
    "        train_dataset = MyDataset(X_train, y_train[column])\n",
    "        test_dataset = MyDataset(X_test, y_test[column]) \n",
    "\n",
    "        # making dataloader\n",
    "        train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                                        batch_size=32,\n",
    "                                        shuffle=True, \n",
    "                                        num_workers=os.cpu_count())\n",
    "        test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                                    batch_size=32, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=os.cpu_count())\n",
    "        \n",
    "        # making model\n",
    "        IN_FEATURES = len(indf.columns)\n",
    "        HIDDEN_UNITS1 = 128\n",
    "        HIDDEN_UNITS2 = 64\n",
    "        OUT_FEATURES = 1\n",
    "        EPOCHS=150\n",
    "        start_lr = 0.001\n",
    "        end_lr = 0.0001\n",
    "        \n",
    "\n",
    "        torch.manual_seed(42) \n",
    "        models[column] = yachtModel8(input_shape=IN_FEATURES, # this is with relu\n",
    "                        hidden_units1=HIDDEN_UNITS1,\n",
    "                        hidden_units2=HIDDEN_UNITS2,\n",
    "                        output_shape=OUT_FEATURES).to(device)\n",
    "\n",
    "        loss_fun = nn.HuberLoss()\n",
    "        optimizer = torch.optim.Adam(params=models[column].parameters(),\n",
    "                                    lr=start_lr)\n",
    "\n",
    "        model_results = train2(model=models[column], \n",
    "                                train_dataloader=train_dataloader, \n",
    "                                test_dataloader=test_dataloader, \n",
    "                                loss_fn=loss_fun, \n",
    "                                optimizer=optimizer, \n",
    "                                epochs=EPOCHS,\n",
    "                                lr_start=start_lr,\n",
    "                                lr_end=end_lr, \n",
    "                                device=device)\n",
    "        final_results2[\"train_loss\"].append(model_results[\"train_loss\"][-1])\n",
    "        final_results2[\"test_loss\"].append(model_results[\"test_loss\"][-1])\n",
    "        \n",
    "        # saving model\n",
    "        torch.save(models[column].state_dict(), \"../models/\" + str(column))\n",
    "    \n",
    "    return final_results2\n",
    "\n",
    "final_results2 = train_models2(X_train=X_train,\n",
    "                             y_train=y_train,\n",
    "                             X_test=X_test, \n",
    "                             y_test=y_test,\n",
    "                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': [0.637565661930708, 0.8637025017536223, 1.121680630944592, 0.9420726663307335, 1.8969165800351997, 1.8721429596310255, 2.036621178310223, 3.475432942895328], 'test_loss': [0.6292900626790033, 0.8038282231108783, 1.4680226371712881, 1.1668217521007747, 1.930725193186982, 2.655328472999677, 3.272263767784589, 3.5069526858525735]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 8 artists>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(final_results2)\n",
    "plt.bar(range(len(final_results2['test_loss'])), final_results2['test_loss']) # predictions for last 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indf.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
